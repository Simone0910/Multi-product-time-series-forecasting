# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dXXsbddNByxlCOlEwRip_ARcuP9JUJre
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

melt = pd.read_csv('luiss-businesscase2-market_data.csv', delimiter=';')
melt = melt.sort_values(by=["PRODUCT_GROUP", 'BRAND', "YEARMONTH"], ascending=[True, True, True])
melt= melt.sort_values(by=["YEARMONTH"], ascending=[True])
melt['YEARMONTH'] = pd.to_datetime(melt['YEARMONTH'], format='%Y%m')
# melt['YEARMONTH'] = melt['YEARMONTH'].dt.strftime('%Y-%m')

df = melt[['YEARMONTH','SALES_OFFLINE','SALES_ONLINE','BRAND','PRODUCT_GROUP','SECTOR']]
df = df.sort_values(by=['PRODUCT_GROUP','BRAND','YEARMONTH'])

##################
################## apple smartphone
# qui facciamo la solita cosa fatta in tutti i codici ovvero slice del dict, filtrare e prendere solo apple smartphone
gruppi = df.groupby((df['BRAND'] != df['BRAND'].shift()).cumsum())
list_df_slices = [gruppo for _, gruppo in gruppi]
gruppi = df.groupby(['BRAND', 'PRODUCT_GROUP'])
dict_df_slices = {name: group for name, group in gruppi}

apple_smartphone = dict_df_slices['APPLE', 'SMARTPHONES']
apple_smartphone.set_index('YEARMONTH', inplace=True)

apple_smartphone_OFF = apple_smartphone.filter(['SALES_OFFLINE', 'PRODUCT_GROUP', 'BRAND', 'SECTOR'])
apple_smartphone_OFF = apple_smartphone_OFF.drop(['PRODUCT_GROUP', 'BRAND', 'SECTOR'], axis = 1)

# splitto
train = apple_smartphone_OFF[apple_smartphone_OFF.index < '2021-02-01']
test = apple_smartphone_OFF[apple_smartphone_OFF.index >= '2021-02-01']

apple_smartphone_OFF.plot(figsize=(12,6))

#scaliamo
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler to the training data
scaler.fit(train)
# Transform the training data using the fitted scaler
scaled_train = scaler.transform(train)
# Transform the test data using the fitted scaler
scaled_test = scaler.transform(test)

scaled_train[:10]



# define generator
n_input = 12
n_features = 1
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)

# Retrieve the first input-output pair from the generator
X, y = generator[0]

# Print the input array (X) after flattening it into a 1D array
print(f'Given the Array: \n{X.flatten()}')

# Print the corresponding output value (y)
print(f'Predict this y: \n {y}')

# Print the shape of the input array (X)
X.shape

# Set the number of input time steps for the TimeSeriesGenerator
n_input = 12

# Create a TimeSeriesGenerator using the scaled training data as both input and target data
# with the specified length and batch size
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)

# define model
model = Sequential()
model.add(LSTM(2000, activation='relu', input_shape=(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.summary()

# fit model
model.fit(generator,epochs=100)



loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)),loss_per_epoch)

# Extract the last 12 time steps from the scaled training data
last_train_batch = scaled_train[-12:]

# Reshape the last_train_batch to the appropriate input format for the model
# The new shape is (1, n_input, n_features)
last_train_batch = last_train_batch.reshape((1, n_input, n_features))

model.predict(last_train_batch)

scaled_test[0]

# Initialize an empty list to store test predictions
test_predictions = []

# Extract the last n_input time steps from the scaled training data
first_eval_batch = scaled_train[-n_input:]

# Reshape the first_eval_batch to the appropriate input format for the model
current_batch = first_eval_batch.reshape((1, n_input, n_features))

# Loop through the test dataset
for i in range(len(test)):

    # Get the prediction value for the current batch
    current_pred = model.predict(current_batch)[0]

    # Append the prediction to the test_predictions list
    test_predictions.append(current_pred)

    # Update the current_batch by removing the first time step and appending the current prediction
    # This allows the model to make predictions based on the most recent time steps
    current_batch = np.append(current_batch[:, 1:, :], [[current_pred]], axis=1)

test_predictions

test.head()

# Invert the scaling transformation on the test_predictions to obtain the true_predictions
true_predictions = scaler.inverse_transform(test_predictions)

# Add the true_predictions as a new column 'Predictions' in the test DataFrame
test['Predictions'] = true_predictions

# plottiamo come performa il modello
test.plot(figsize=(10,5))

from sklearn.metrics import mean_absolute_percentage_error
MAPE = mean_absolute_percentage_error(test['SALES_OFFLINE'],test['Predictions'])
print('MAPE' , MAPE)